{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.keys import Keys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_cats():\n",
    "    cats=[]\n",
    "    sub_cat=[]\n",
    "    urls=[]\n",
    "    dicto={}\n",
    "    url = ('https://www.alibaba.com/Products')\n",
    "    r=requests.get(url)\n",
    "    web_content = BeautifulSoup(r.text, 'lxml')\n",
    "    divs = web_content.findAll('div',class_='sub-item')\n",
    "    for div in divs:\n",
    "        cat=div.find('h4').find('a').text.replace(\"\\n\", \"\").strip()\n",
    "        sub_cats=div.findAll('a')\n",
    "        sub_cats.pop(0)\n",
    "        for el in sub_cats:\n",
    "            urls.append(el[\"href\"])\n",
    "            sub_cat.append(el.text.replace(\"\\n\", \"\").strip())\n",
    "            cats.append(cat)\n",
    "    data=[np.array(cats),np.array(sub_cat),np.array(urls)]\n",
    "    result=pd.DataFrame(data).transpose()\n",
    "    result.columns = ['Category', 'Sub Category', 'URL']\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all pages of the catalog\n",
    "def get_all_catalog_product(url_catalogue):\n",
    "    i=1\n",
    "    urls=[]\n",
    "    url_catalogue=url_catalogue.replace('pid','cid')\n",
    "    if (url_catalogue.find('catalog')==-1):\n",
    "        url_catalogue=url_catalogue[:23]+'catalog/'+url_catalogue[23:]\n",
    "    r=requests.get(url_catalogue)\n",
    "    web_content = BeautifulSoup(r.text, 'lxml')\n",
    "    while (not (web_content.findAll('div',class_='right')) and i<=1):\n",
    "        url=url_catalogue+'?page='+str(i)\n",
    "        r=requests.get(url)\n",
    "        web_content = BeautifulSoup(r.text, 'lxml')\n",
    "        urls.append(get_all_products(url))\n",
    "        i=i+1\n",
    "    return urls\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#infinite scroll with selenium to get all the products of the catalog\n",
    "def get_all_products(url):\n",
    "    urls=[]\n",
    "    print(url)\n",
    "    chromedriver_path= \"C:/Users/ASUS/Desktop/chromedriver\"\n",
    "    driver = webdriver.Chrome(chromedriver_path)\n",
    "    driver.get(url)\n",
    "    driver.find_element_by_tag_name('body').send_keys(Keys.END) \n",
    "    time.sleep(10)\n",
    "    page_source = driver.page_source\n",
    "    web_content = BeautifulSoup(page_source, 'lxml')\n",
    "    divs=web_content.findAll('div',class_='offer-list-wrapper')\n",
    "    for div in divs:\n",
    "        i=0\n",
    "        for a in div.findAll('a',href=True):\n",
    "            if (a[\"href\"][:5]==\"//www\"):\n",
    "                if (i%2==0):\n",
    "                    urls.append(a[\"href\"][2:])\n",
    "                i=i+1\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "#single product \n",
    "names=[]\n",
    "urls=[]\n",
    "transactions=[]\n",
    "quantities=[]\n",
    "buyers=[]\n",
    "category=[]\n",
    "sub_category=[]\n",
    "for i in range(0,len(list_url)):\n",
    "    for k in range(0,len(list_url[i][0])):\n",
    "        url='https://'+list_url[i][0][k]\n",
    "        r=requests.get(url)\n",
    "        web_content = BeautifulSoup(r.text, 'lxml')\n",
    "        spans = web_content.findAll('span',class_='tab-name')\n",
    "        trouve=False\n",
    "        for span in spans:\n",
    "            if (span.text==\"Transactions\"):\n",
    "                trouve=True\n",
    "        if (trouve):\n",
    "            chromedriver_path= \"C:/Users/ASUS/Desktop/chromedriver\"\n",
    "            driver = webdriver.Chrome(chromedriver_path)\n",
    "            driver.get(url)\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            driver.find_element_by_xpath(\"//span[@title='Transactions']\").click()\n",
    "            #WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//span[@title='Transactions']\"))).click()\n",
    "            time.sleep(10)\n",
    "            page_source = driver.page_source\n",
    "            web_content = BeautifulSoup(page_source, 'lxml')\n",
    "            spans = web_content.findAll('span',class_='overview-item-content')\n",
    "            name= web_content.findAll('h1',class_='module-pdp-title')\n",
    "            if (len(spans)==3):\n",
    "                if (spans[2].text>='2'):\n",
    "                    transactions.append(spans[0].text)\n",
    "                    quantities.append(spans[1].text)\n",
    "                    buyers.append(spans[2].text)\n",
    "                    names.append(name[0].text)\n",
    "                    urls.append(url)\n",
    "                    category.append(categories[0])\n",
    "                    sub_category.append(sub_categories[i])\n",
    "frames=[urls,names,transactions,quantities,buyers,category,sub_category]\n",
    "final=pd.DataFrame(frames)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames=[urls,names,transactions,quantities,buyers,category,sub_category]\n",
    "final=pd.DataFrame(frames)\n",
    "final=final.transpose()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
